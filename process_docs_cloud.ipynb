{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'kmpg-case'\n",
    "location = 'eu'  \n",
    "processor_display_name = 'PDF_PROCESSOR_EU' \n",
    "processor_type = 'OCR_PROCESSOR'  \n",
    "processor_version = 'rc' \n",
    "mime_type = 'application/pdf' \n",
    "processor_id = 'fde971a6ca78aafa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A PROCESSOR on Google Cloud FOR OCR SCANNING DOCUMENTS \n",
    "# CREATE THIS ONLY 1 TIME \n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.api_core.exceptions import FailedPrecondition\n",
    "from google.cloud import documentai\n",
    "\n",
    "def create_processor(\n",
    "    project_id: str, location: str, processor_display_name: str, processor_type: str\n",
    "):\n",
    "    # You must set the api_endpoint if you use a location other than 'us', e.g.:\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    # The full resource name of the location\n",
    "    # e.g.: projects/project_id/locations/location\n",
    "    parent = client.common_location_path(project_id, location)\n",
    "\n",
    "    # Create a processor\n",
    "    processor = client.create_processor(\n",
    "        parent=parent,\n",
    "        processor=documentai.Processor(\n",
    "            display_name=processor_display_name, type_=processor_type\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Print the processor information\n",
    "    print(f\"Processor Name: {processor.name}\")\n",
    "    print(f\"Processor Display Name: {processor.display_name}\")\n",
    "    print(f\"Processor Type: {processor.type_}\")\n",
    "\n",
    "# ENABLE THE PROCESSOR in the cloud\n",
    "# Used only once\n",
    "\n",
    "def enable_processor(project_id: str, location: str, processor_id: str):\n",
    "    # You must set the api_endpoint if you use a location other than 'us', e.g.:\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    # The full resource name of the location\n",
    "    # e.g.: projects/project_id/locations/location/processors/processor_id\n",
    "    processor_name = client.processor_path(project_id, location, processor_id)\n",
    "    request = documentai.EnableProcessorRequest(name=processor_name)\n",
    "\n",
    "    # Make EnableProcessor request\n",
    "    try:\n",
    "        operation = client.enable_processor(request=request)\n",
    "\n",
    "        # Print operation name\n",
    "        print(operation.operation.name)\n",
    "        # Wait for operation to complete\n",
    "        operation.result()\n",
    "    # Cannot enable a processor that is already enabled\n",
    "    except FailedPrecondition as e:\n",
    "        print(e.message)\n",
    "\n",
    "#create_processor( project_id, location, processor_display_name, processor_type)\n",
    "#enable_processor(project_id,location,processor_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTTEXT LANGUAGE detection \n",
    "import fasttext as ft\n",
    "\n",
    "# Load the pretrained model\n",
    "ft_model = ft.load_model(\"lid.176.ftz\")\n",
    "\n",
    "def fasttext_language_predict(text, model = ft_model):\n",
    "\n",
    "  text = text.replace('\\n', \" \")\n",
    "  prediction = model.predict([text])\n",
    "\n",
    "  return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS DOCUMENT CORE\n",
    "\n",
    "def process_document(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    processor_version: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    ") -> documentai.Document:\n",
    "    # You must set the api_endpoint if you use a location other than 'us', e.g.:\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    # The full resource name of the processor version\n",
    "    # e.g. projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}\n",
    "    # You must create processors before running sample code.\n",
    "    name = client.processor_version_path(\n",
    "        project_id, location, processor_id, processor_version\n",
    "    )\n",
    "\n",
    "    # Read the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Load Binary Data into Document AI RawDocument Object\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    return result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS DOCUMENT with OCR\n",
    " \n",
    "from typing import Sequence\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def process_document_ocr(    project_id: str,    location: str,    processor_id: str,\n",
    "    processor_version: str,    file_path: str,    document_id: str,    mime_type: str,) -> None:\n",
    "\n",
    "    # Online processing request to Document AI\n",
    "    document = process_document(project_id, location, processor_id, processor_version, file_path, mime_type)\n",
    "    text = document.text \n",
    "    \n",
    "    # Write document as text file\n",
    "    with open(f'processed_data/{document_id}.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(text) \n",
    "    # make new_row\n",
    "    cols=[] \n",
    "    new_row=[document_id]\n",
    "    cols.append('document_id')\n",
    "\n",
    "    # Detect language of text\n",
    "    language=fasttext_language_predict(text ,model=ft_model) \n",
    "    language=language[0][0][0][-2:] \n",
    "\n",
    "    # Add language to new_row\n",
    "    new_row.append(language)\n",
    "    cols.append('language')\n",
    "\n",
    "    # Counter for paragraphs\n",
    "    base=int(0)\n",
    "\n",
    "    # Get all paragraphs from all pages\n",
    "    for page in document.pages:\n",
    "        new_row, cols, x = print_paragraphs(page.paragraphs, text, new_row, cols, base)\n",
    "        base+=x\n",
    "\n",
    "    # Write new_now as a dataframe  \n",
    "    new_df = pd.DataFrame([new_row], columns=cols)\n",
    "\n",
    "    # Return dataframe\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def print_paragraphs(paragraphs: Sequence[documentai.Document.Page.Paragraph], text: str, new_row: list, cols: list, base: int) -> None:\n",
    "    for x in range (len(paragraphs)):\n",
    "        paragraph_text=layout_to_text(paragraphs[x].layout, text)\n",
    "        new_row.append(paragraph_text)\n",
    "        cols.append(f'p{base+x+1}')\n",
    "    return new_row , cols, x+1\n",
    "\n",
    "def layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Document AI identifies text in different parts of the document by their\n",
    "    offsets in the entirety of the document's text. This function converts\n",
    "    offsets to a string.\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    # If a text segment spans several lines, it will\n",
    "    # be stored in different text segments.\n",
    "    for segment in layout.text_anchor.text_segments:\n",
    "        start_index = int(segment.start_index)\n",
    "        end_index = int(segment.end_index)\n",
    "        response += text[start_index:end_index]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count pages in .pdf\n",
    "import PyPDF2\n",
    "\n",
    "def get_nr_of_pages(file):\n",
    "    readpdf = PyPDF2.PdfFileReader(file)\n",
    "    totalpages = readpdf.numPages\n",
    "    return totalpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alive_progress\n",
    "import os\n",
    "from os import listdir\n",
    "import shutil\n",
    "\n",
    "# Create new dataframe with paragraphs from all documents\n",
    "df=pd.DataFrame()\n",
    "\n",
    "path='Bilingual/Nederlands/'\n",
    "counter=int(0)    \n",
    "files_in_path=listdir(path)\n",
    "\n",
    "for file in files_in_path:     \n",
    "    if file[-4:]=='.pdf':\n",
    "        counter+=1\n",
    "        document_id=file\n",
    "        file_path=os.path.join(path,document_id)\n",
    "\n",
    "        # Max page size on Google Cloud = 10\n",
    "        if get_nr_of_pages(file_path)<=10:\n",
    "            print(f\"[{counter}/{len(files_in_path)}] Processing {document_id} ...   \",end='\\r') \n",
    "            new_df=process_document_ocr(project_id,location,processor_id,processor_version,file_path,document_id,mime_type)\n",
    "            df = pd.concat([df, new_df], ignore_index = True)\n",
    "            print(f\"Processing {document_id} : Done\",end='\\r') \n",
    "            df.to_csv('FR_doc_paragraphs.csv', index=True) \n",
    "        \n",
    "        else:    \n",
    "            # Copy remaining file \n",
    "            src_path = file_path\n",
    "            dst_path = os.path.join('processed_data','error')\n",
    "            dst_file= os.path.join(dst_path,document_id)\n",
    "            shutil.copy(src_path, dst_file)\n",
    "            print(f'{document_id}: Too Large', end=\"\\r\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "googenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54a7e325ecc84fa625d8a3ed09038798785ca9ae88cae94e662f38988cc00c57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

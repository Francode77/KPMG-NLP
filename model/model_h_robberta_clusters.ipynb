{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows.\n",
    "!pip install \"tensorflow-gpu<2.11\"  \n",
    "!pip install --upgrade pip\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install rake_nltk \n",
    "!pip install -U scikit-learn\n",
    "!pip install transformers\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install seaborn\n",
    "!pip install pdfminer.six\n",
    "!pip install pdfrw\n",
    "# For error : AttributeError: module 'keras.engine.data_adapter' has no attribute 'expand_1d'\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df_model=pd.read_csv('../csv/NL_document_targets_deg_to_ratio_filtered.csv',sep=';',index_col=[0])\n",
    "\n",
    "print (f'Number of encoded columns: {len(df_model.columns)-2}')\n",
    "encoded_columns=len(df_model.columns)-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine clusters from the first (n) ohe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_1 = pd.get_dummies(df_model['1'])\n",
    "y_2 = pd.get_dummies(df_model['2'])\n",
    "y_3 = pd.get_dummies(df_model['3'])\n",
    "y_4 = pd.get_dummies(df_model['4'])\n",
    "y_5 = pd.get_dummies(df_model['5'])\n",
    "y_6 = pd.get_dummies(df_model['6'])\n",
    "y_7 = pd.get_dummies(df_model['7'])\n",
    "y_8 = pd.get_dummies(df_model['8'])\n",
    "y_9 = pd.get_dummies(df_model['9'])\n",
    "y_10 = pd.get_dummies(df_model['10'])\n",
    "\n",
    "y=pd.concat([y_1,y_2,y_3,y_4,y_5,y_6,y_7,y_8,y_9,y_10],axis=1,join='inner') \n",
    "\n",
    "y = y.groupby(level=0,axis=1).sum()\n",
    "y_unique = y.loc[:,~y.columns.duplicated()].copy() \n",
    "\n",
    "y=y_unique \n",
    "y.head(-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def ideal_n_clusters():\n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(1,100)\n",
    "    for k in K:\n",
    "        km = KMeans(n_clusters=k)\n",
    "        km = km.fit(y)\n",
    "        Sum_of_squared_distances.append(km.inertia_)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "#ideal_n_clusters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA #Principal Component Analysis\n",
    "import numpy as np\n",
    "\n",
    "kmeans = KMeans(n_clusters=100)\n",
    "kmeans.fit(y)\n",
    "\n",
    "#Find which cluster each data-point belongs to\n",
    "clusters = kmeans.predict(y)\n",
    "\n",
    "#Add the cluster vector to our DataFrame, y\n",
    "y[\"cluster\"] = clusters\n",
    "df_model[\"cluster\"] = clusters\n",
    " \n",
    "#plotX is a DataFrame containing 5000 values sampled randomly from X\n",
    "plot_y = pd.DataFrame(np.array(y))\n",
    "\n",
    "#Rename plotX's columns since it was briefly converted to an np.array above\n",
    "plot_y.columns = y.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn imports\n",
    "from sklearn.decomposition import PCA #Principal Component Analysis\n",
    "from sklearn.manifold import TSNE #T-Distributed Stochastic Neighbor Embedding\n",
    "from sklearn.preprocessing import StandardScaler #used for 'Feature Scaling'\n",
    "\n",
    "#plotly imports\n",
    "import plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA with one principal component\n",
    "pca_1d = PCA(n_components=1)\n",
    "\n",
    "#PCA with two principal components\n",
    "pca_2d = PCA(n_components=2)\n",
    "\n",
    "#PCA with three principal components\n",
    "pca_3d = PCA(n_components=3)\n",
    "\n",
    "#PCA with 4 principal components\n",
    "pca_4d = PCA(n_components=4)\n",
    "\n",
    "#PCA with 4 principal components\n",
    "pca_5d = PCA(n_components=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This DataFrame holds that single principal component mentioned above\n",
    "PCs_1d = pd.DataFrame(pca_1d.fit_transform(plot_y.drop([\"cluster\"], axis=1)))\n",
    "\n",
    "#This DataFrame contains the two principal components that will be used\n",
    "#for the 2-D visualization mentioned above\n",
    "PCs_2d = pd.DataFrame(pca_2d.fit_transform(plot_y.drop([\"cluster\"], axis=1)))\n",
    "\n",
    "#And this DataFrame contains three principal components that will aid us\n",
    "#in visualizing our clusters in 3-D\n",
    "PCs_3d = pd.DataFrame(pca_3d.fit_transform(plot_y.drop([\"cluster\"], axis=1)))\n",
    "\n",
    "#And this DataFrame contains three principal components that will aid us\n",
    "#in visualizing our clusters in 4-D\n",
    "PCs_4d = pd.DataFrame(pca_4d.fit_transform(plot_y.drop([\"cluster\"], axis=1)))\n",
    "\n",
    "#And this DataFrame contains three principal components that will aid us\n",
    "#in visualizing our clusters in 4-D\n",
    "PCs_5d = pd.DataFrame(pca_5d.fit_transform(plot_y.drop([\"cluster\"], axis=1)))\n",
    "\n",
    "#Rename the columns of these newly created DataFrames:\n",
    "PCs_1d.columns = [\"PC1_1d\"]\n",
    "\n",
    "#\"PC1_2d\" means: 'The first principal component of the components created for 2-D visualization, by PCA.'\n",
    "#And \"PC2_2d\" means: 'The second principal component of the components created for 2-D visualization, by PCA.'\n",
    "PCs_2d.columns = [\"PC1_2d\", \"PC2_2d\"]\n",
    "\n",
    "PCs_3d.columns = [\"PC1_3d\", \"PC2_3d\", \"PC3_3d\"]\n",
    "PCs_4d.columns = [\"PC1_4d\", \"PC2_4d\", \"PC3_4d\", \"PC4_4d\"]\n",
    "PCs_5d.columns = [\"PC1_4d\", \"PC2_4d\", \"PC3_4d\", \"PC4_4d\", \"PC5_5d\"]\n",
    "\n",
    "#plot_y = pd.concat([plot_y,PCs_1d,PCs_2d,PCs_3d], axis=1, join='inner')\n",
    "#plot_y = pd.concat([plot_y,PCs_1d,PCs_2d,PCs_3d,PCs_4d], axis=1, join='inner')\n",
    "plot_y = pd.concat([plot_y,PCs_1d,PCs_2d,PCs_3d,PCs_4d,PCs_5d], axis=1, join='inner')\n",
    "\n",
    "# create one new column for plotX so that we can use it for 1-D visualization.\n",
    "plot_y[\"dummy\"] = 0\n",
    "\n",
    "#Note that all of the DataFrames below are sub-DataFrames of 'plotX'.\n",
    "#This is because we intend to plot the values contained within each of these DataFrames.\n",
    "\n",
    "cluster0 = plot_y[plot_y[\"cluster\"] == 0]\n",
    "cluster1 = plot_y[plot_y[\"cluster\"] == 1]\n",
    "cluster2 = plot_y[plot_y[\"cluster\"] == 2]\n",
    "cluster3 = plot_y[plot_y[\"cluster\"] == 3]\n",
    "cluster4 = plot_y[plot_y[\"cluster\"] == 4]\n",
    "\n",
    "#This is needed so we can display plotly plots properly\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions for building the 2-D plot\n",
    "# https://www.kaggle.com/code/minc33/visualizing-high-dimensional-clusters/notebook\n",
    "#trace1 is for 'Cluster 0'\n",
    "trace1 = go.Scatter(\n",
    "                    x = cluster0[\"PC1_2d\"],\n",
    "                    y = cluster0[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 0\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace2 is for 'Cluster 1'\n",
    "trace2 = go.Scatter(\n",
    "                    x = cluster1[\"PC1_2d\"],\n",
    "                    y = cluster1[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 1\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace3 is for 'Cluster 2'\n",
    "trace3 = go.Scatter(\n",
    "                    x = cluster2[\"PC1_2d\"],\n",
    "                    y = cluster2[\"PC2_2d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 2\",\n",
    "                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "data = [trace1, trace2, trace3]\n",
    "\n",
    "title = \"Visualizing Clusters in Two Dimensions Using PCA\"\n",
    "\n",
    "layout = dict(title = title,\n",
    "              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n",
    "              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n",
    "             )\n",
    "\n",
    "fig = dict(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instructions for building the 3-D plot\n",
    "\n",
    "#trace1 is for 'Cluster 0'\n",
    "trace1 = go.Scatter3d(\n",
    "                    x = cluster0[\"PC1_3d\"],\n",
    "                    y = cluster0[\"PC2_3d\"],\n",
    "                    z = cluster0[\"PC3_3d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 0\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 255, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace2 is for 'Cluster 1'\n",
    "trace2 = go.Scatter3d(\n",
    "                    x = cluster1[\"PC1_3d\"],\n",
    "                    y = cluster1[\"PC2_3d\"],\n",
    "                    z = cluster1[\"PC3_3d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 1\",\n",
    "                    marker = dict(color = 'rgba(255, 128, 2, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace3 is for 'Cluster 2'\n",
    "trace3 = go.Scatter3d(\n",
    "                    x = cluster2[\"PC1_3d\"],\n",
    "                    y = cluster2[\"PC2_3d\"],\n",
    "                    z = cluster2[\"PC3_3d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 2\",\n",
    "                    marker = dict(color = 'rgba(0, 255, 200, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace3 is for 'Cluster 3'\n",
    "trace4 = go.Scatter3d(\n",
    "                    x = cluster3[\"PC1_3d\"],\n",
    "                    y = cluster3[\"PC2_3d\"],\n",
    "                    z = cluster3[\"PC3_3d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 3\",\n",
    "                    marker = dict(color = 'rgba(128, 10, 200, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "#trace3 is for 'Cluster 4'\n",
    "trace5 = go.Scatter3d(\n",
    "                    x = cluster4[\"PC1_3d\"],\n",
    "                    y = cluster4[\"PC2_3d\"],\n",
    "                    z = cluster4[\"PC3_3d\"],\n",
    "                    mode = \"markers\",\n",
    "                    name = \"Cluster 4\",\n",
    "                    marker = dict(color = 'rgba(2, 128, 0, 0.8)'),\n",
    "                    text = None)\n",
    "\n",
    "data = [trace1, trace2, trace3,trace4,trace5]\n",
    "\n",
    "title = \"Visualizing Clusters in Three Dimensions Using PCA\"\n",
    "\n",
    "layout = dict(title = title,\n",
    "              xaxis= dict(title= 'PC1',ticklen= 5,zeroline= False),\n",
    "              yaxis= dict(title= 'PC2',ticklen= 5,zeroline= False)\n",
    "             )\n",
    "\n",
    "fig = dict(data = data, layout = layout)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this we need the  Jaccard Distance \n",
    "\n",
    "Because eucledian distance also takes into account the 0 correlation\n",
    "\n",
    "Jaccard counts the 1 correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard():\n",
    "    import scipy\n",
    "\n",
    "    jaccard = scipy.spatial.distance.cdist(y, y,  \n",
    "                                        metric='jaccard')\n",
    "    user_distance = pd.DataFrame(jaccard, columns=y.index.values,  \n",
    "                                index=y.index.values)\n",
    "\n",
    "    user_distance.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the full text shorter with NLTK\n",
    "\n",
    "- Get ranked phrases with Rake_NLTK<br>\n",
    "- Select only these terms which identify as NL with fasttext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTTEXT LANGUAGE detection \n",
    "import fasttext as ft\n",
    "\n",
    "# Load the pretrained model\n",
    "ft_model = ft.load_model(\"../preprocessing/lid.176.ftz\")\n",
    "\n",
    "def fasttext_language_predict(text, model = ft_model):\n",
    "\n",
    "  text = text.replace('\\n', \" \")\n",
    "  prediction = model.predict([text])\n",
    "\n",
    "  return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the text in dataframe shorter \n",
    "# And replace df_model['text'] with this shorter condensed version\n",
    "\n",
    "def shorten_text():\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    # Add custom stopwords\n",
    "    stopwords = stopwords.words('dutch')\n",
    "\n",
    "    custom_stopwords=['per','waar','waarvoor','wegens','wanneer','gevolg','gevolge','voorbehoud','erratum','correctie','sommige','betreffende','maatregel','procedure','stelsel','sector','organisatie','excl','aanv','adv','art','artikel','hoofdstuk','XII','XI','IX','VII','VI','V', '2020','2019','2018','2021','2022','dag','dagen','uur','uren','jaar','jaarlijks','maand','maanden','januari','februari','maart','april','mei','juni','juli','augustus','september','oktober','november','december''volgt','voordat','behoudt','beschouwd','bepaald','gedaan','leiden','zullen','gaan']\n",
    "    stopwords.extend(custom_stopwords)\n",
    "    \n",
    "\n",
    "    from rake_nltk import Metric, Rake\n",
    "    rake_nltk_var = Rake(language='dutch',stopwords=stopwords,include_repeated_phrases=False,ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO,min_length=1,max_length=3)\n",
    "    import re\n",
    "\n",
    "    for i in range(len(df_model)):\n",
    "        full_text=df_model[\"text\"][i]\n",
    "        \n",
    "        rake_nltk_var.extract_keywords_from_text(full_text)\n",
    "        \n",
    "        phrases_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "\n",
    "        # Keyword_extracted is a list, so split them into seperate words for the columns \n",
    "    \n",
    "        phrases=set(phrases_extracted)\n",
    "\n",
    "        # First column will be document_id, so put it as first element\n",
    "        doc_keywords=[]\n",
    "\n",
    "        # Exclude errors like 'e' as keyword\n",
    "        for phrase in phrases:\n",
    "            if len(phrase)>3:\n",
    "                detected_language=fasttext_language_predict(phrase, model = ft_model)[0][0][0][-2:]  \n",
    "                if detected_language.upper()=='NL':\n",
    "                    doc_keywords.append(phrase)\n",
    "\n",
    "\n",
    "        short_text=doc_keywords\n",
    "        #print (len(short_text))\n",
    "\n",
    "        #print (short_text)\n",
    "        string_text=str()\n",
    "        for keywords in short_text:\n",
    "            string_text+=keywords\n",
    "            string_text+=' '\n",
    "\n",
    "        #print (string_text)\n",
    "        df_model[\"text\"][i]=string_text\n",
    " \n",
    "\n",
    "shorten_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters=df_model\n",
    "df_clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the phrase/keyword columns\n",
    "for i in range(1,encoded_columns+1):\n",
    "    df_clusters.drop(f'{i}',axis=1,inplace=True)\n",
    "df_clusters.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the new dataframe to .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clusters.to_csv('../csv/TRAIN_DS_Clusters.csv', sep=\";\", index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now redefine y as target\n",
    "y should now be the clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "df_model=pd.read_csv('../csv/TRAIN_DS_Clusters.csv', sep=\";\") \n",
    "df_model.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Convert y from pd Series to pd dataframe!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model[\"text\"].astype(str).tolist()\n",
    "y = df_model['cluster'].to_frame() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train test val \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split Train and Validation data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.15, random_state=4,stratify=y)\n",
    "\n",
    "# Keep some data for inference (testing)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15, random_state=9,stratify=y)\n",
    "print ('X_train',len(X_train))\n",
    "print ('X_test',len(X_test))\n",
    "print ('X_val',len(X_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate GPU with maximal memory allocation\n",
    "\n",
    "print (tf.__version__)\n",
    "# Ref: https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) # Notice here\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\", num_labels=len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, max_length=128, truncation=True, padding=True)\n",
    "val_encodings   = tokenizer(X_val,   max_length=128, truncation=True, padding=True)\n",
    "test_encodings  = tokenizer(X_test,  max_length=128, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AdamW optimizer\n",
    "\n",
    "from official.nlp import optimization \n",
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 5e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0.0,\n",
    "    axis=-1,\n",
    "    reduction=\"auto\",\n",
    "    name=\"categorical_crossentropy\",\n",
    ")\n",
    "metrics = tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    " \n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS =6\n",
    "history=model.fit(\n",
    "    train_dataset.batch(BATCH_SIZE) ,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset.batch(BATCH_SIZE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'kpmg_model_h_robberta_dutch_softmax'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open ('../processed_data/NL/NL_312-2019-012471.txt') as f:\n",
    "    text=f.read() \n",
    "text = clean_text(text)\n",
    "#print (text)\n",
    "\n",
    "#not enough values to unpack (expected 2, got 1)\n",
    "#input_ids = input_ids.unsqueeze(0)\n",
    "#attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "filename = 'NLP_model.sav' \n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "encodings = tokenizer([text], truncation=True, padding=True)\n",
    "ds = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "\n",
    "print (ds)\n",
    "ds=ds.batch(1, drop_remainder=True)\n",
    "print(ds)\n",
    "predictions = model.predict(ds)\n",
    "\n",
    "mapping = {i: name for i, name in enumerate(y.columns)}\n",
    "\n",
    "import numpy as np\n",
    "print(mapping[np.argmax(predictions[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "def plot_loss(history):\n",
    "# Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color='red', label='Train Loss')\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color='green', label='Val Loss',\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "  \n",
    "    plt.legend()\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "from sklearn.metrics import roc_curve,confusion_matrix,auc\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_cm(y_true, y_pred, title):\n",
    "    ''''\n",
    "    input y_true-Ground Truth Labels\n",
    "          y_pred-Predicted Value of Model\n",
    "          title-What Title to give to the confusion matrix\n",
    "    \n",
    "    Draws a Confusion Matrix for better understanding of how the model is working\n",
    "    \n",
    "    return None\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    figsize=(10,10)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
    " \n",
    "val_dataset=val_dataset.batch(1, drop_remainder=True) \n",
    "y_predict=model.predict(val_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y_predict[0])\n",
    "y_predict[float(y_predict[0])> 0.5] = 1\n",
    "y_predict[y_predict <= 0.5] = 0\n",
    "plot_cm(y_val, y_predict, 'Performance-Confusion Matrix')\n",
    "plot_cm()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "406a731f6857295c713001c7c2465c908d27c40a29d9b077acd9b1cc8f3de530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

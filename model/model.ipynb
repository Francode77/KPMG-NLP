{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugginface transformers\n",
    "This notebook aims to classify the CLA documents based on a pre-trained NLP model from HuggingFace. \n",
    "\n",
    "We have extracted the most important keywords from the metadata in `make_targets_from_metadata.ipynb` and will use this set of keywords as target for a pre-trained model.\n",
    "\n",
    "The results are very dissapointing, so another method is used in the `clustering.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "# Huggingface\n",
    "from transformers import TFDistilBertForSequenceClassification  \n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from rake_nltk import Metric, Rake\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# FastText\n",
    "import fasttext as ft\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tqdm as notebook_tqdm\n",
    "from ipywidgets import FloatProgress\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language detection\n",
    "\n",
    "We use the handy and very effective language detection library fasttext. This will give us and idea in what language the document is written. It is used in the short_text function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# FASTTEXT LANGUAGE detection \n",
    "# Load the pretrained model\n",
    "ft_model = ft.load_model(\"../model/pretrained/lid.176.ftz\")\n",
    "\n",
    "def fasttext_language_predict(text, model = ft_model):\n",
    "\n",
    "  text = text.replace('\\n', \" \")\n",
    "  prediction = model.predict([text])\n",
    "\n",
    "  return prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2. Shortening the text\n",
    "\n",
    "1. Get ranked phrases with Rake_NLTK\n",
    "2. Do the language detection\n",
    "3. Replace the original text with the shortened phrases \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_text(full_text):\n",
    "\n",
    "    # Add custom stopwords\n",
    "    stopwords_list = stopwords.words('dutch')\n",
    "\n",
    "    custom_stopwords=['per','waar','waarvoor','wegens','wanneer','gevolg','gevolge','voorbehoud','erratum','correctie','sommige','betreffende','maatregel','procedure','stelsel','sector','organisatie','excl','aanv','adv','art','artikel','hoofdstuk','XII','XI','IX','VII','VI','V', '2020','2019','2018','2021','2022','dag','dagen','uur','uren','jaar','jaarlijks','maand','maanden','januari','februari','maart','april','mei','juni','juli','augustus','september','oktober','november','december''volgt','voordat','behoudt','beschouwd','bepaald','gedaan','leiden','zullen','gaan']\n",
    "    stopwords_list.extend(custom_stopwords)\n",
    "    \n",
    "    rake_nltk_var = Rake(language='dutch',stopwords=stopwords_list,include_repeated_phrases=False,ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO,min_length=1,max_length=3)\n",
    "\n",
    "    rake_nltk_var.extract_keywords_from_text(full_text)    \n",
    "    phrases_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "\n",
    "    # Keyword_extracted is a list, so split them into seperate words for the columns \n",
    "    phrases=set(phrases_extracted)\n",
    "\n",
    "    # First column will be document_id, so put it as first element\n",
    "    doc_keywords=[]\n",
    "\n",
    "    # Exclude keywords with lenght < 4\n",
    "    for phrase in phrases:\n",
    "        if len(phrase)>3:\n",
    "            detected_language=fasttext_language_predict(phrase, model = ft_model)[0][0][0][-2:]  \n",
    "            if detected_language.upper()=='NL':\n",
    "                doc_keywords.append(phrase)\n",
    "\n",
    "    short_text=doc_keywords\n",
    "    string_text=str()\n",
    "\n",
    "    for keywords in short_text:\n",
    "        string_text+=keywords\n",
    "        string_text+=' '\n",
    "\n",
    "    return string_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the text in dataframe shorter \n",
    "# And replace df_model['text'] with this shorter condensed version\n",
    "\n",
    "def modify_df(df):\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        full_text=df[\"text\"][i]\n",
    "        \n",
    "        string_text=shorten_text(full_text)\n",
    "\n",
    "        df.loc[i, 'text'] = string_text\n",
    "        return df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X and y\n",
    "\n",
    "For X we will use the shortened text, instead of the full document text.\n",
    "\n",
    "For y we load the labels as found by our `make_targets_from_metadata.ipynb` notebook in preprocessing folder.\n",
    "\n",
    "For this notebook we will only use the first column, which hosts the most important label from the nltk extraction.\n",
    "\n",
    "Then we split our training data into train, test and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "X_train 140\n",
      "X_test 40\n",
      "X_val 60\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('../csv/CLA_targets_NL.csv',sep=';')\n",
    "df_model=modify_df(df)\n",
    "\n",
    "df_model=df_model[:200]\n",
    "X = df_model[\"text\"].tolist()\n",
    "\n",
    "y_1 = pd.get_dummies(df_model['1'])\n",
    "y_2 = pd.get_dummies(df_model['2'])\n",
    "y_3 = pd.get_dummies(df_model['3'])\n",
    "y_4 = pd.get_dummies(df_model['4'])\n",
    "y_5 = pd.get_dummies(df_model['5'])\n",
    "y_6 = pd.get_dummies(df_model['6'])\n",
    "y_7 = pd.get_dummies(df_model['7'])\n",
    "y_8 = pd.get_dummies(df_model['8'])\n",
    "y_9 = pd.get_dummies(df_model['9'])\n",
    "y_10 = pd.get_dummies(df_model['10'])\n",
    "y=pd.concat([y_1,y_2,y_3,y_4,y_5,y_6,y_7,y_8,y_9,y_10],axis=1,join='inner')\n",
    "y=pd.concat([y_1],axis=1,join='inner')\n",
    " \n",
    "y = y.groupby(level=0,axis=1).sum()\n",
    "y_unique = y.loc[:,~y.columns.duplicated()].copy() \n",
    "\n",
    "y=y_unique\n",
    "\n",
    "# Split Train and Validation data\n",
    "\n",
    "#print (len(X))\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=43)\n",
    "#print ('X_train_val',len(X_train_val))\n",
    "#print ('X_test',len(X_test))\n",
    "\n",
    "print (len(X))\n",
    "# Keep some data for inference (testing)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=43)\n",
    "print ('X_train',len(X_train))\n",
    "print ('X_test',len(X_test))\n",
    "print ('X_val',len(X_val))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The labels \n",
    "\n",
    "Now we have the deduced labels for our model to train on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aanvullende pensioenen</th>\n",
       "      <th>actieve werknemer</th>\n",
       "      <th>bedrijfstoeslag</th>\n",
       "      <th>bonuskostenvergoedingen</th>\n",
       "      <th>ecocheques</th>\n",
       "      <th>eindeloopbaandagenoudere werknemers</th>\n",
       "      <th>feestdagen</th>\n",
       "      <th>fondsen</th>\n",
       "      <th>functieclassificatie</th>\n",
       "      <th>landingsbanen</th>\n",
       "      <th>...</th>\n",
       "      <th>overlijdensvergoeding</th>\n",
       "      <th>overuren</th>\n",
       "      <th>risicogroepen</th>\n",
       "      <th>swtopzegging</th>\n",
       "      <th>swtstelsel</th>\n",
       "      <th>syndicale afvaardiging</th>\n",
       "      <th>syndicale premie</th>\n",
       "      <th>syndicale vormingmaatregel</th>\n",
       "      <th>uitzendarbeidveiligheid</th>\n",
       "      <th>vergoedingen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>195 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     aanvullende pensioenen  actieve werknemer  bedrijfstoeslag  \\\n",
       "0                         0                  0                0   \n",
       "1                         0                  0                0   \n",
       "2                         0                  0                0   \n",
       "3                         0                  0                0   \n",
       "4                         1                  0                0   \n",
       "..                      ...                ...              ...   \n",
       "190                       0                  0                1   \n",
       "191                       1                  0                0   \n",
       "192                       0                  0                0   \n",
       "193                       0                  0                0   \n",
       "194                       0                  1                0   \n",
       "\n",
       "     bonuskostenvergoedingen  ecocheques  eindeloopbaandagenoudere werknemers  \\\n",
       "0                          0           0                                    0   \n",
       "1                          0           0                                    0   \n",
       "2                          0           0                                    0   \n",
       "3                          0           0                                    0   \n",
       "4                          0           0                                    0   \n",
       "..                       ...         ...                                  ...   \n",
       "190                        0           0                                    0   \n",
       "191                        0           0                                    0   \n",
       "192                        0           0                                    0   \n",
       "193                        0           1                                    0   \n",
       "194                        0           0                                    0   \n",
       "\n",
       "     feestdagen  fondsen  functieclassificatie  landingsbanen  ...  \\\n",
       "0             0        0                     0              0  ...   \n",
       "1             0        0                     0              0  ...   \n",
       "2             0        0                     0              0  ...   \n",
       "3             0        0                     0              0  ...   \n",
       "4             0        0                     0              0  ...   \n",
       "..          ...      ...                   ...            ...  ...   \n",
       "190           0        0                     0              0  ...   \n",
       "191           0        0                     0              0  ...   \n",
       "192           0        1                     0              0  ...   \n",
       "193           0        0                     0              0  ...   \n",
       "194           0        0                     0              0  ...   \n",
       "\n",
       "     overlijdensvergoeding  overuren  risicogroepen  swtopzegging  swtstelsel  \\\n",
       "0                        0         0              0             0           0   \n",
       "1                        0         0              0             0           0   \n",
       "2                        0         0              0             0           0   \n",
       "3                        0         0              0             0           0   \n",
       "4                        0         0              0             0           0   \n",
       "..                     ...       ...            ...           ...         ...   \n",
       "190                      0         0              0             0           0   \n",
       "191                      0         0              0             0           0   \n",
       "192                      0         0              0             0           0   \n",
       "193                      0         0              0             0           0   \n",
       "194                      0         0              0             0           0   \n",
       "\n",
       "     syndicale afvaardiging  syndicale premie  syndicale vormingmaatregel  \\\n",
       "0                         0                 0                           1   \n",
       "1                         0                 0                           1   \n",
       "2                         0                 0                           0   \n",
       "3                         0                 0                           0   \n",
       "4                         0                 0                           0   \n",
       "..                      ...               ...                         ...   \n",
       "190                       0                 0                           0   \n",
       "191                       0                 0                           0   \n",
       "192                       0                 0                           0   \n",
       "193                       0                 0                           0   \n",
       "194                       0                 0                           0   \n",
       "\n",
       "     uitzendarbeidveiligheid  vergoedingen  \n",
       "0                          0             0  \n",
       "1                          0             0  \n",
       "2                          0             0  \n",
       "3                          0             1  \n",
       "4                          0             0  \n",
       "..                       ...           ...  \n",
       "190                        0             0  \n",
       "191                        0             0  \n",
       "192                        0             0  \n",
       "193                        0             0  \n",
       "194                        0             0  \n",
       "\n",
       "[195 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.to_csv('../csv/columns.csv',sep=';')\n",
    "y.head(-5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model selection\n",
    "\n",
    "We choose distilbert for sequence classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_transform', 'vocab_layer_norm', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_19', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(y))) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Here we tokenize the texts and create the  train, test and validation sets. \n",
    "\n",
    "We try various max_lengths. For the purpose of demonstration we keep it at 12 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, max_length=12, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val, max_length=12, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, max_length=12, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    "))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile model\n",
    "\n",
    "### Metrics, Loss function, optimizer\n",
    "\n",
    "We try a few metrics, accuracy f1_score and binaryaccuracy lead to serious overfitting, AUC aswell. The best results in terms of overfitting is not using any metrics at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  19994     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66,973,466\n",
      "Trainable params: 66,973,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZER =  tf.keras.optimizers.Adam(learning_rate= 1e-5)\n",
    "LOSS = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "METRICS=['Accuracy']\n",
    "METRICS = [tf.keras.metrics.BinaryAccuracy()]\n",
    "METRICS=[tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve='ROC',\n",
    "    summation_method='interpolation',\n",
    "    name=None,\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=True,\n",
    "    num_labels=len(set(y)),\n",
    "    label_weights=None,\n",
    "    from_logits=True\n",
    ")]\n",
    " \n",
    "METRICS=[tfa.metrics.F1Score(\n",
    "    average=None, threshold=None, name='f1_score', dtype=None, num_classes=len(set(y))\n",
    ")]\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "70/70 [==============================] - 19s 135ms/step - loss: 0.5819 - f1_score: 0.0167 - val_loss: 0.4600 - val_f1_score: 0.0026\n",
      "Epoch 2/8\n",
      "70/70 [==============================] - 9s 125ms/step - loss: 0.3897 - f1_score: 0.0213 - val_loss: 0.3217 - val_f1_score: 0.0090\n",
      "Epoch 3/8\n",
      "70/70 [==============================] - 8s 109ms/step - loss: 0.2790 - f1_score: 0.0223 - val_loss: 0.2354 - val_f1_score: 0.0090\n",
      "Epoch 4/8\n",
      "70/70 [==============================] - 8s 111ms/step - loss: 0.2093 - f1_score: 0.0247 - val_loss: 0.1872 - val_f1_score: 0.0090\n",
      "Epoch 5/8\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1710 - f1_score: 0.0244 - val_loss: 0.1630 - val_f1_score: 0.0090\n",
      "Epoch 6/8\n",
      "70/70 [==============================] - 8s 120ms/step - loss: 0.1521 - f1_score: 0.0229 - val_loss: 0.1508 - val_f1_score: 0.0090\n",
      "Epoch 7/8\n",
      "70/70 [==============================] - 9s 123ms/step - loss: 0.1405 - f1_score: 0.0250 - val_loss: 0.1452 - val_f1_score: 0.0090\n",
      "Epoch 8/8\n",
      "70/70 [==============================] - 8s 118ms/step - loss: 0.1355 - f1_score: 0.0301 - val_loss: 0.1422 - val_f1_score: 0.0090\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS =8\n",
    "history=model.fit(\n",
    "    train_dataset.batch(BATCH_SIZE) ,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset.batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(\"[^a-zA-Z\\'\\-éòóôëè]\", \" \", text) \n",
    "  return \" \".join(word_tokenize(text)[:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify\n",
    "\n",
    "Let's see what predictions are, we print out the 1st,2nd and 3th highest label scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10202-2018-012766.txt 6250 uitzendarbeidveiligheid opleiding syndicale afvaardiging\n",
      "10202-2020-013175.txt 2735 uitzendarbeidveiligheid opleiding swtstelsel\n",
      "10205-2018-004963.txt 4326 uitzendarbeidveiligheid opleiding syndicale afvaardiging\n",
      "10206-2019-003872.txt 32313 uitzendarbeidveiligheid opleiding syndicale afvaardiging\n",
      "10206-2020-000814.txt 4597 uitzendarbeidveiligheid opleiding syndicale afvaardiging\n",
      "10206-2021-015270.txt 4752 uitzendarbeidveiligheid opleiding syndicale afvaardiging\n",
      "10207-2018-013171.txt 10922 uitzendarbeidveiligheid opleiding syndicale afvaardiging\n",
      "10207-2018-013172.txt 4119 uitzendarbeidveiligheid opleiding syndicale afvaardiging\n",
      "10207-2018-013174.txt 7428 uitzendarbeidveiligheid opleiding syndicale afvaardiging\n",
      "10209-2021-015509.txt 998 uitzendarbeidveiligheid opleiding swtstelsel\n",
      "104-2019-010269.txt 11774 uitzendarbeidveiligheid opleiding swtstelsel\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "\n",
    "for root, dirs, files in os.walk('../data/processed/NL/'):\n",
    " \n",
    "    for file in files:\n",
    "\n",
    "        with open(os.path.join(root, file), 'r',encoding='utf-8') as f: \n",
    "            text=f.read() \n",
    "            text = clean_text(text)\n",
    "            text= shorten_text(text)\n",
    "            encodings = tokenizer([text], truncation=True, padding=True)\n",
    "            ds = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "            predictions = model.predict(ds)\n",
    "            \n",
    "            class_indices = np.argsort(predictions[0])[::-1]  # get the indices of the probabilities in descending order\n",
    "            highest_index = class_indices[0][0]  # select the index of the class with the highest probability\n",
    "            second_index = class_indices[0][3]  # select the index of the class with the 2nd highest probability\n",
    "            third_index = class_indices[0][9]  # select the index of the class with the 3th highest probability\n",
    "            mapping = {i: name for i, name in enumerate(y.columns)}\n",
    "                \n",
    "            print(file, len(text), mapping[highest_index], mapping[second_index], mapping[third_index])\n",
    "            \n",
    "            count+=1\n",
    "            if count >10: \n",
    "                break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "As we can see this model classification is not capable of assigning the right labels to the text. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0ef82e47554a784be4aef5e6d53d04690795db09a31e9d8c0cc3106ddef404c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

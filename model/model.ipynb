{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows.\\n# !pip install \"tensorflow<2.11\" \\n!pip install tensorflow==2.3\\n!pip install --upgrade pip\\n!pip install pandas\\n!pip install nltk\\n!pip install rake_nltk \\n!pip install -U scikit-learn\\n!pip install transformers\\n!pip install matplotlib\\n!pip install numpy\\n!pip install seaborn\\n# For error : AttributeError: module \\'keras.engine.data_adapter\\' has no attribute \\'expand_1d\\'\\n!pip install --upgrade git+https://github.com/huggingface/transformers.git'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#TensorFlow 2.10 was the last TensorFlow release that supported GPU on native-Windows.\n",
    "!pip install \"tensorflow<2.11\"  \n",
    "!pip install --upgrade pip\n",
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install rake_nltk \n",
    "!pip install -U scikit-learn\n",
    "!pip install transformers\n",
    "!pip install matplotlib\n",
    "!pip install numpy\n",
    "!pip install seaborn\n",
    "# For error : AttributeError: module 'keras.engine.data_adapter' has no attribute 'expand_1d'\n",
    "!pip install --upgrade git+https://github.com/huggingface/transformers.git\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Theranet\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(\"[^a-zA-Z\\'\\-éòóôëè]\", \" \", text) \n",
    "  return \" \".join(word_tokenize(text)[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "X_train 140\n",
      "X_test 40\n",
      "X_val 60\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_model=pd.read_csv('../csv/NL_document_full_targets.csv',sep=';')\n",
    "df_model=pd.read_csv('../csv/NL_document_targets.csv',sep=';')\n",
    "\n",
    "df_model=df_model[:200]\n",
    "X = df_model[\"text\"].tolist()\n",
    "\n",
    "y_1 = pd.get_dummies(df_model['1'])\n",
    "y_2 = pd.get_dummies(df_model['2'])\n",
    "y_3 = pd.get_dummies(df_model['3'])\n",
    "y_4 = pd.get_dummies(df_model['4'])\n",
    "y_5 = pd.get_dummies(df_model['5'])\n",
    "y_6 = pd.get_dummies(df_model['6'])\n",
    "y_7 = pd.get_dummies(df_model['7'])\n",
    "y_8 = pd.get_dummies(df_model['8'])\n",
    "y_9 = pd.get_dummies(df_model['9'])\n",
    "y_10 = pd.get_dummies(df_model['10'])\n",
    "y=pd.concat([y_1,y_2,y_3,y_4,y_5,y_6,y_7,y_8,y_9,y_10],axis=1,join='inner')\n",
    " \n",
    "y = y.groupby(level=0,axis=1).sum()\n",
    "y_unique = y.loc[:,~y.columns.duplicated()].copy()\n",
    "drop_columns=['excl','e']\n",
    "y_unique.drop(['excl','e'], axis=1, inplace=True)\n",
    "\n",
    "y=y_unique\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split Train and Validation data\n",
    "\n",
    "#print (len(X))\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "#print ('X_train_val',len(X_train_val))\n",
    "#print ('X_test',len(X_test))\n",
    "\n",
    "print (len(X))\n",
    "# Keep some data for inference (testing)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print ('X_train',len(X_train))\n",
    "print ('X_test',len(X_test))\n",
    "print ('X_val',len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()\n",
    "y.to_csv('columns.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 5499712028016202363\n",
      "xla_global_id: -1\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print (tf.__version__)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Theranet\\becode\\GCloud\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\", num_labels=len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['activation_13', 'vocab_transform', 'vocab_layer_norm', 'vocab_projector']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'classifier', 'dropout_57']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import TFDistilBertForSequenceClassification  \n",
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, max_length=12, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val, max_length=12, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, max_length=12, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMai  multiple                 66362880  \n",
      " nLayer)                                                         \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  83821     \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67,037,293\n",
      "Trainable params: 67,037,293\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "OPTIMIZER =  tf.keras.optimizers.Adam(learning_rate=1)\n",
    "LOSS = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "#METRICS = ['accuracy']\n",
    "\n",
    "#model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "12/12 [==============================] - 21s 2s/step - loss: 15438.9424 - val_loss: 11534.1201\n",
      "Epoch 2/2\n",
      "12/12 [==============================] - 22s 2s/step - loss: 4368.3550 - val_loss: 3707.8311\n"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "BATCH_SIZE = 12\n",
    "EPOCHS =2\n",
    "history=model.fit(\n",
    "    train_dataset.batch(BATCH_SIZE) ,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset.batch(BATCH_SIZE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<TensorSliceDataset element_spec={'input_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(512,), dtype=tf.int32, name=None)}>\n",
      "<BatchDataset element_spec={'input_ids': TensorSpec(shape=(1, 512), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(1, 512), dtype=tf.int32, name=None)}>\n",
      "1/1 [==============================] - 1s 920ms/step\n",
      "werkloosheid\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "with open ('../processed_data/NL/NL_312-2019-012471.txt') as f:\n",
    "    text=f.read() \n",
    "text = clean_text(text)\n",
    "#print (text)\n",
    "\n",
    "#not enough values to unpack (expected 2, got 1)\n",
    "#input_ids = input_ids.unsqueeze(0)\n",
    "#attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "# remove max_length\n",
    "\n",
    "encodings = tokenizer([text], truncation=True, padding=True)\n",
    "ds = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "\n",
    "print (ds)\n",
    "ds=ds.batch(1, drop_remainder=True)\n",
    "print(ds)\n",
    "predictions = model.predict(ds)\n",
    "\n",
    "mapping = {i: name for i, name in enumerate(y.columns)}\n",
    "\n",
    "import numpy as np\n",
    "print(mapping[np.argmax(predictions[0])])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0ef82e47554a784be4aef5e6d53d04690795db09a31e9d8c0cc3106ddef404c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We now redefine y as target\n",
    "y should now be the clusters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "df_model=pd.read_csv('../csv/TRAIN_DS_Clusters.csv', sep=\";\") \n",
    "df_model.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create OHE dataframe for targets\n",
    "\n",
    "or\n",
    "*Convert y from pd Series to pd dataframe!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "X = df_model[\"text\"].astype(str).tolist()\n",
    "\n",
    "#y = df_model['cluster'].to_frame() \n",
    "y = pd.get_dummies(df_model['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make train test val \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split Train and Validation data\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Keep some data for inference (testing)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.15)\n",
    "print ('X_train',len(X_train))\n",
    "print ('X_test',len(X_test))\n",
    "print ('X_val',len(X_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate GPU with maximal memory allocation\n",
    "\n",
    "print (tf.__version__)\n",
    "# Ref: https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)]) # Notice here\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "\n",
    "model = TFRobertaForSequenceClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\", num_labels=len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, max_length=128, truncation=True, padding=True)\n",
    "val_encodings   = tokenizer(X_val,   max_length=128, truncation=True, padding=True)\n",
    "test_encodings  = tokenizer(X_test,  max_length=128, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val\n",
    "))\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    ")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create AdamW optimizer\n",
    "\n",
    "from official.nlp import optimization \n",
    "epochs = 5\n",
    "steps_per_epoch = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 5e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set LOSS & METRICS\n",
    "IF we have targets as **integers** and not as one_hot_encoded dataframe, we have to use\n",
    "\n",
    "**SparseCategoricalCrossentropy** instead of *CategoricalCrossentropy*\n",
    "and \n",
    "\n",
    "**SparseCategoricalAccuracy** instead of *CategoricalAccuracy*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Compile the model\n",
    "\n",
    "loss= tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    reduction=\"auto\",\n",
    "    name=\"sparse_categorical_crossentropy\",\n",
    ") \n",
    "metrics = tf.keras.metrics.SparseCategoricalAccuracy(name=\"sparse_categorical_accuracy\", dtype=None)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "model.summary()\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But we are using OHE columns\n",
    "Because loss turns into nan with above loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "\n",
    "loss=tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=False,\n",
    "    label_smoothing=0.0,\n",
    "    axis=-1,\n",
    "    reduction=\"auto\",\n",
    "    name=\"categorical_crossentropy\",\n",
    ")\n",
    "metrics = tf.keras.metrics.CategoricalAccuracy(name=\"categorical_accuracy\", dtype=None)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model\n",
    " \n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS =6\n",
    "history=model.fit(\n",
    "    train_dataset.batch(BATCH_SIZE) ,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset.batch(BATCH_SIZE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['categorical_accuracy']\n",
    "val_acc = history_dict['val_categorical_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# r is for \"solid red line\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'kpmg_model_h_robberta_dutch_softmax'\n",
    "saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))\n",
    "\n",
    "model.save(saved_model_path, include_optimizer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.saved_model.load(saved_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open ('../processed_data/NL/NL_312-2019-012471.txt') as f:\n",
    "    text=f.read() \n",
    "text = clean_text(text)\n",
    "#print (text)\n",
    "\n",
    "#not enough values to unpack (expected 2, got 1)\n",
    "#input_ids = input_ids.unsqueeze(0)\n",
    "#attention_mask = attention_mask.unsqueeze(0)\n",
    "\n",
    "filename = 'NLP_model.sav' \n",
    "#loaded_model = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "\n",
    "encodings = tokenizer([text], truncation=True, padding=True)\n",
    "ds = tf.data.Dataset.from_tensor_slices(dict(encodings))\n",
    "\n",
    "print (ds)\n",
    "ds=ds.batch(1, drop_remainder=True)\n",
    "print(ds)\n",
    "predictions = model.predict(ds)\n",
    "\n",
    "mapping = {i: name for i, name in enumerate(y.columns)}\n",
    "\n",
    "import numpy as np\n",
    "print(mapping[np.argmax(predictions[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rcParams['figure.figsize'] = (12, 10)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "def plot_loss(history):\n",
    "# Use a log scale to show the wide range of values.\n",
    "    plt.semilogy(history.epoch,  history.history['loss'],\n",
    "               color='red', label='Train Loss')\n",
    "    plt.semilogy(history.epoch,  history.history['val_loss'],\n",
    "          color='green', label='Val Loss',\n",
    "          linestyle=\"--\")\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "  \n",
    "    plt.legend()\n",
    "\n",
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "from sklearn.metrics import roc_curve,confusion_matrix,auc\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_cm(y_true, y_pred, title):\n",
    "    ''''\n",
    "    input y_true-Ground Truth Labels\n",
    "          y_pred-Predicted Value of Model\n",
    "          title-What Title to give to the confusion matrix\n",
    "    \n",
    "    Draws a Confusion Matrix for better understanding of how the model is working\n",
    "    \n",
    "    return None\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    figsize=(10,10)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\n",
    "    annot = np.empty_like(cm).astype(str)\n",
    "    nrows, ncols = cm.shape\n",
    "    for i in range(nrows):\n",
    "        for j in range(ncols):\n",
    "            c = cm[i, j]\n",
    "            p = cm_perc[i, j]\n",
    "            if i == j:\n",
    "                s = cm_sum[i]\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\n",
    "            elif c == 0:\n",
    "                annot[i, j] = ''\n",
    "            else:\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n",
    "    cm.index.name = 'Actual'\n",
    "    cm.columns.name = 'Predicted'\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    plt.title(title)\n",
    "    sns.heatmap(cm, cmap= \"YlGnBu\", annot=annot, fmt='', ax=ax)\n",
    " \n",
    "val_dataset=val_dataset.batch(1, drop_remainder=True) \n",
    "y_predict=model.predict(val_dataset, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y_predict[0])\n",
    "y_predict[float(y_predict[0])> 0.5] = 1\n",
    "y_predict[y_predict <= 0.5] = 0\n",
    "plot_cm(y_val, y_predict, 'Performance-Confusion Matrix')\n",
    "plot_cm()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "406a731f6857295c713001c7c2465c908d27c40a29d9b077acd9b1cc8f3de530"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = 'kmpg-case'\n",
    "location = 'eu'  \n",
    "processor_display_name = 'PDF_PROCESSOR_EU' \n",
    "processor_type = 'OCR_PROCESSOR'  \n",
    "processor_version = 'rc' \n",
    "mime_type = 'application/pdf' \n",
    "processor_id = 'fde971a6ca78aafa'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "pip install --upgrade google-cloud-documentai\n",
    "pip install fasttext\n",
    "pip install pyPDF2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE A Document AI PROCESSOR on Google Cloud FOR OCR SCANNING DOCUMENTS \n",
    "# CREATE THIS ONLY 1 TIME \n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.api_core.exceptions import FailedPrecondition\n",
    "from google.cloud import documentai\n",
    "\n",
    "def create_processor(\n",
    "    project_id: str, location: str, processor_display_name: str, processor_type: str\n",
    "):\n",
    "    # You must set the api_endpoint if you use a location other than 'us', e.g.:\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    # The full resource name of the location\n",
    "    # e.g.: projects/project_id/locations/location\n",
    "    parent = client.common_location_path(project_id, location)\n",
    "\n",
    "    # Create a processor\n",
    "    processor = client.create_processor(\n",
    "        parent=parent,\n",
    "        processor=documentai.Processor(\n",
    "            display_name=processor_display_name, type_=processor_type\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    # Print the processor information\n",
    "    print(f\"Processor Name: {processor.name}\")\n",
    "    print(f\"Processor Display Name: {processor.display_name}\")\n",
    "    print(f\"Processor Type: {processor.type_}\")\n",
    "\n",
    "# ENABLE THE PROCESSOR in the cloud\n",
    "# Used only once\n",
    "\n",
    "def enable_processor(project_id: str, location: str, processor_id: str):\n",
    "    # You must set the api_endpoint if you use a location other than 'us', e.g.:\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    # The full resource name of the location\n",
    "    # e.g.: projects/project_id/locations/location/processors/processor_id\n",
    "    processor_name = client.processor_path(project_id, location, processor_id)\n",
    "    request = documentai.EnableProcessorRequest(name=processor_name)\n",
    "\n",
    "    # Make EnableProcessor request\n",
    "    try:\n",
    "        operation = client.enable_processor(request=request)\n",
    "\n",
    "        # Print operation name\n",
    "        print(operation.operation.name)\n",
    "        # Wait for operation to complete\n",
    "        operation.result()\n",
    "    # Cannot enable a processor that is already enabled\n",
    "    except FailedPrecondition as e:\n",
    "        print(e.message)\n",
    "\n",
    "#create_processor( project_id, location, processor_display_name, processor_type)\n",
    "#enable_processor(project_id,location,processor_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASTTEXT LANGUAGE detection \n",
    "import fasttext as ft\n",
    "\n",
    "# Load the pretrained model\n",
    "ft_model = ft.load_model(\"lid.176.ftz\")\n",
    "\n",
    "def fasttext_language_predict(text, model = ft_model):\n",
    "\n",
    "  text = text.replace('\\n', \" \")\n",
    "  prediction = model.predict([text])\n",
    "\n",
    "  return prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS DOCUMENT AI CORE\n",
    "# This function links Document AI from Google to the Google processor we just made\n",
    "\n",
    "def process_document(\n",
    "    project_id: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    processor_version: str,\n",
    "    file_path: str,\n",
    "    mime_type: str,\n",
    ") -> documentai.Document:\n",
    "    # You must set the api_endpoint if you use a location other than 'us', e.g.:\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    # The full resource name of the processor version\n",
    "    # e.g. projects/{project_id}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}\n",
    "    # You must create processors before running sample code.\n",
    "    name = client.processor_version_path(\n",
    "        project_id, location, processor_id, processor_version\n",
    "    )\n",
    "\n",
    "    # Read the file into memory\n",
    "    with open(file_path, \"rb\") as image:\n",
    "        image_content = image.read()\n",
    "\n",
    "    # Load Binary Data into Document AI RawDocument Object\n",
    "    raw_document = documentai.RawDocument(content=image_content, mime_type=mime_type)\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(name=name, raw_document=raw_document)\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    return result.document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROCESS DOCUMENT with OCR\n",
    " \n",
    "from typing import Sequence\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.cloud import documentai\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def process_document_ocr(    project_id: str,    location: str,    processor_id: str,\n",
    "    processor_version: str,    file_path: str,    document_id: str,    mime_type: str, output_path) -> None:\n",
    "\n",
    "    # Online processing request to Document AI\n",
    "    document = process_document(project_id, location, processor_id, processor_version, file_path, mime_type)\n",
    "    text = document.text \n",
    "    \n",
    "    # Clean the filename\n",
    "    document_id_filename=re.sub('.pdf','',document_id)\n",
    "    \n",
    "    # Write document as text file\n",
    "    with open(f'{output_path}/{document_id_filename}.txt', 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(text) \n",
    "    # make new_row\n",
    "    cols=[] \n",
    "    new_row=[document_id]\n",
    "    cols.append('document_id')\n",
    "\n",
    "    # detect if pdf has multiple parts (if original has > 10 pages)\n",
    "    if '__' in document_id:\n",
    "        position=document_id.rfind('__')\n",
    "        sub_part=document_id[position:]\n",
    "    else:\n",
    "        sub_part=''\n",
    "    \n",
    "    new_row.append(sub_part)\n",
    "    cols.append('sub_part')\n",
    "\n",
    "    # Detect language of text\n",
    "    language=fasttext_language_predict(text ,model=ft_model) \n",
    "    language=language[0][0][0][-2:] \n",
    "\n",
    "    # Add language to new_row\n",
    "    new_row.append(language)\n",
    "    cols.append('language')\n",
    "\n",
    "    # Counter for paragraphs\n",
    "    base=int(0)\n",
    "\n",
    "    # Get all paragraphs from all pages\n",
    "    for page in document.pages:\n",
    "        new_row, cols, x = print_paragraphs(page.paragraphs, text, new_row, cols, base)\n",
    "        base+=x\n",
    "\n",
    "    # Write new_row as a dataframe  \n",
    "    new_df = pd.DataFrame([new_row], columns=cols)\n",
    "\n",
    "    # Return dataframe\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def print_paragraphs(paragraphs: Sequence[documentai.Document.Page.Paragraph], text: str, new_row: list, cols: list, base: int) -> None:\n",
    "    if len(paragraphs)!=0:\n",
    "        for x in range (len(paragraphs)):\n",
    "            # Get the text of this paragraph\n",
    "            paragraph_text=layout_to_text(paragraphs[x].layout, text)\n",
    "            # Add text in new_row\n",
    "            new_row.append(paragraph_text)\n",
    "            # Create paragraph column for dataframe\n",
    "            cols.append(f'p{base+x+1}')\n",
    "\n",
    "        return new_row , cols, x+1\n",
    "    else:\n",
    "        return new_row , cols, 0\n",
    "\n",
    "def layout_to_text(layout: documentai.Document.Page.Layout, text: str) -> str:\n",
    "    \"\"\"\n",
    "    Document AI identifies text in different parts of the document by their\n",
    "    offsets in the entirety of the document's text. This function converts\n",
    "    offsets to a string.\n",
    "    \"\"\"\n",
    "    response = \"\"\n",
    "    # If a text segment spans several lines, it will\n",
    "    # be stored in different text segments.\n",
    "    for segment in layout.text_anchor.text_segments:\n",
    "        start_index = int(segment.start_index)\n",
    "        end_index = int(segment.end_index)\n",
    "        response += text[start_index:end_index]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count pages in .pdf\n",
    "import PyPDF2\n",
    "\n",
    "def get_nr_of_pages(file):\n",
    "    readpdf = PyPDF2.PdfFileReader(file)\n",
    "    totalpages = readpdf.numPages\n",
    "    return totalpages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function checks if files are still to be processed\n",
    "# These documents have already been processed into a .txt file\n",
    "# Make a list of these documents to not feed them to Google Document AI\n",
    " \n",
    "from os import listdir \n",
    "import os\n",
    "\n",
    "def check_if_processed(file_to_check):\n",
    "    \n",
    "    # Work in this folder\n",
    "    path='../new_processed_data/too_large/fixed/txt' \n",
    "    files_in_path=listdir(path)\n",
    "    documents=[]\n",
    "\n",
    "    # Make a list of all processed documents\n",
    "    for file in files_in_path:    \n",
    "\n",
    "        if file.find('__') != -1 and os.path.isdir(file)!=True:\n",
    "\n",
    "            page=re.findall(r'__(.*)',file) \n",
    "            page=page[0][:-4]\n",
    "\n",
    "            document_id=re.sub(page[0],'',file)\n",
    "            document_id=re.sub(f'__{page}','',file)\n",
    "            document_id=file[3:] \n",
    "            document_id=re.sub('__(.*)','',document_id)\n",
    "            documents.append(document_id)\n",
    "\n",
    "    # Check if file_to_check is in already processed documents list\n",
    "    document_nr=file_to_check\n",
    "    document_nr=file_to_check[3:]\n",
    "    document_nr=re.sub('\\.pdf','',document_nr)\n",
    "    document_nr=re.sub('__(.*)','',document_nr) \n",
    " \n",
    "    if document_nr not in documents:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This functions processes all .pdf files in input directory\n",
    "\n",
    "from joblib import delayed, Parallel\n",
    "import os\n",
    "from os import listdir\n",
    "import shutil\n",
    "\n",
    "# Parallel function\n",
    "def process_pdfs(output_path,files_in_path,n_jobs,n):\n",
    "    x=int(n)\n",
    "    df=pd.DataFrame()\n",
    "    while x< len(files_in_path):\n",
    "\n",
    "        file=files_in_path[x]\n",
    " \n",
    "        # Check if file is .pdf\n",
    "        if file[-4:]=='.pdf': \n",
    "\n",
    "            document_id=file\n",
    "            check=check_if_processed(file) \n",
    "            if check==False:\n",
    "                file_path=os.path.join(path,document_id)\n",
    "\n",
    "                # Max page size on Google Cloud = 10\n",
    "                if get_nr_of_pages(file_path)<=10:\n",
    "\n",
    "                    print(f\"[{x}/{len(files_in_path)}] Processing {document_id} ...   \",end='\\r') \n",
    "\n",
    "                    # Run the process on the cloud\n",
    "                    new_df=process_document_ocr(project_id,location,processor_id,processor_version,file_path,document_id,mime_type,output_path)\n",
    "\n",
    "                    # Add new output to df\n",
    "                    df = pd.concat([df, new_df], ignore_index = True)\n",
    "\n",
    "                    print(f\"Processing {document_id} : Done                                             \",end='\\r') \n",
    "                \n",
    "                else:    \n",
    "                    # Copy pdf file with > 10 pages to error folder\n",
    "                    src_path = file_path\n",
    "                    dst_path = os.path.join(output_path,'too_large')\n",
    "                    dst_file= os.path.join(dst_path,document_id)\n",
    "                    shutil.copy(src_path, dst_file)\n",
    "                    print(f'{document_id}: Too Large', end=\"\\r\")\n",
    "        x+=n_jobs\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this function to process files in input folder\n",
    "# This function will save the processed .txt files in /processed_data\n",
    "# And save a .csv file with the paragraphs and detected language\n",
    "\n",
    "# NOTICE: afterwards, execute split_max_page_10.ipynb to split the too large .pdfs\n",
    "# and run these through this script again\n",
    "\n",
    "input_path='../new_processed_data/too_large/fixed' \n",
    "output_path='../new_processed_data/too_large/fixed/txt'\n",
    "output_csv='../NL_doc_paragraphs_new_processed_too_large_fixed_extra.csv'\n",
    "\n",
    "# Check if output directories are present\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "if not os.path.exists(os.path.join(output_path,'too_large')):\n",
    "    os.mkdir(os.path.join(output_path,'too_large'))\n",
    "\n",
    "# Run script with n_jobs\n",
    "n_jobs=10\n",
    "\n",
    "# Work in this folder\n",
    "path=input_path\n",
    "files_in_path=listdir(path)\n",
    "\n",
    "# Create parallel pool\n",
    "delayed_funcs = [delayed(process_pdfs)(output_path,files_in_path,n_jobs,n) for n in range (0,n_jobs)]\n",
    "parallel_pool = Parallel(n_jobs=n_jobs, require='sharedmem')\n",
    "df_list=parallel_pool(delayed_funcs)\n",
    "\n",
    "# Create result dataframe\n",
    "df=pd.DataFrame()\n",
    "\n",
    "# Add all the dataframes to the result dataframe\n",
    "for x in range (n_jobs):\n",
    "    df=pd.concat([df,df_list[x]])\n",
    "\n",
    "# Write the result dataframe \n",
    "df.to_csv(output_csv, index=True) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GCloud",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0ef82e47554a784be4aef5e6d53d04690795db09a31e9d8c0cc3106ddef404c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

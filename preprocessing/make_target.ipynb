{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AAA=[[\"Lonen\",\n",
    "\"Minimumuur- en maandlonen\",\"Jongerenlonen (niet studentenlonen)\",\"Studentenlonen\",\"Gewaarborgd gemiddeld minimumloon of -inkomen (ggmmi cao 43)\",\"Variabel loon\",\"Stukloon\",\"Huisarbeid\",\"Reële lonen\",\"Indexeringsbepalingen\",\"Loonsverhogingen\",\"Openingsclausules\",\"Loonnorm\",\"Loon - administratieve en uitbetalingsmodaliteiten\"]\n",
    ",[\n",
    "\"Functieclassificatie\",\"Functieclassificatie\",\"Evaluatie van de werknemer - procedure\",\"Loonkloof\"],\n",
    "\n",
    "[\"Arbeidsduur\",\"Wekelijkse arbeidsduur of op jaarbasis\",\"Dagelijkse arbeidsduur\",\"Arbeidsduurvermindering\",\"Betaalde verlofdag en feestdag\",\"Anciënniteitsverlof (andere dan eindeloopbaandagen)\",\"Bijkomende verlofdagen volgens leeftijd/eindeloopbaandagen\",\"Schenken van conventioneel verlof\",\"Arbeidsduurmodaliteiten\",\"Rusttijden / pauzes\",\"Wachttijd\"],\n",
    "[\"Arbeidsduurflexibiliteit en overuren - arbeidsorganisatie\",\"Overuren - vergoeding en/of recuperatie\",\"Overuren - quotum\",\"Loopbaansparen\",\n",
    "\"Telewerk\",\"Tijdelijke arbeid (geen uitzendarbeid)\",\"Uitzendarbeid\",\"Deeltijdse arbeid\",\"Loopbaanonderbreking\",\"Tijdskrediet/loopbaanvermindering\",\"Landingsbanen\",\"Klein verlet\",\"Verlof om dwingende reden\",\"Ouderschapsverlof\",\"Verlof om persoonlijke redenen\"]\n",
    ",[\"Aanwervingen en opleidingen\",\"Aanwerving\",\"Opleiding (excl. syndicale vorming)\",\"Opleiding voor jongeren\",\"Economische werkloosheid bedienden\"],\n",
    "\n",
    "[\"Werkgelegenheidsmaatregelen en doelgroepen\",\"Maatregel voor onderneming in moeilijkheden/herstructurering-niet swt\",\"Ontslagvergoeding bij sluiting of collectief ontslag / afscheidspremie\",\"Oudere werknemers:anciënniteitsverlof,-toeslag,-premie/afscheidspremie\",\"Oudere werknemers: eindeloopbaandagen en verlofdagen volgens leeftijd\",\"Oudere werknemers: landingsbanen\",\"Oudere werknemers: leeftijdsgebonden toeslagen\",\n",
    "\"Oudere werknemers: werkgelegenheidsmaatregelen\",\"Risicogroepen\"],\n",
    "\n",
    "[\n",
    "\"Veiligheid, gezondheid en welzijn\",\n",
    "\"Sociale maribel\",\n",
    "\"Veiligheid op het werk\",\n",
    "\"Welzijn op het werk\",\n",
    "\"Discriminatie\",\n",
    "\"Ziekte/ongeval/overlijden\" ],[\n",
    "\"Beëindiging van de arbeidsovereenkomst en werkloosheid\",\n",
    "\"Economische werkloosheid bedienden\",\n",
    "\"Werkloosheid (andere dan economische werkloosheid voor bedienden)\",\n",
    "\"Brugpensioen\",\n",
    "\"Stelsel van werkloosheid met bedrijfstoeslag (swt)\",\n",
    "\"Opzegging/ontslag - organisatie en procedure\",\n",
    "\"Opzegging/ontslag - vergoeding\",\n",
    "\"Opzegging/ontslag - werkgelegenheidsmaatregelen\" ]\n",
    ",\n",
    "[\n",
    "\"Syndicale vertegenwoordiging en premie\",\n",
    "\"Ontslag beschermde werknemers\",\n",
    "\"Syndicale afvaardiging\",\n",
    "\"Syndicale vorming\",\n",
    "\"Syndicale premie\"]\n",
    ",\n",
    "[\n",
    "\"Sociale fondsen\",\n",
    "\"Fondsen voor bestaanszekerheid\",\n",
    "\"Sociale fondsen, andere dan fondsen voor bestaanszekerheid\"]\n",
    ",[\n",
    "\n",
    "\"Werking comité/onderneming\",\n",
    "\"Werking paritair comité\",\n",
    "\"Arbeidsreglement\",\n",
    "\"Conventionele overlegorganen (andere dan sa, or, cpbw)\",\n",
    "\"Syndicale afvaardiging\",\n",
    "\"Ondernemingsraad\",\n",
    "\"Comité voor preventie en bescherming op het werk\",\n",
    "\"Bijzondere cao bij wijziging ressort pc\"]\n",
    ",[\n",
    "\"Diverse onderwerpen: overgang personeel, harmonisering, detachering\",\n",
    "\"Overgang van personeel naar een andere onderneming\",\n",
    "\"Harmonisering statuut arbeiders/bedienden\",\n",
    "\"Detachering\"]\n",
    "]\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(AAA)\n",
    "for x in range(len(AAA)):\n",
    "    for y in range(len(AAA[x])):\n",
    "        df[x][y]=str(AAA[x][y])\n",
    "df.to_csv('../csv/CLA_themes.csv',sep=';')\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df_m=pd.read_csv(\"../data/CLA_meta_from_2018.csv\") \n",
    "df = pd.read_csv(\"../csv/NL_doc_paragraphs_extra_mean.csv\")\n",
    "df_meta = pd.read_csv(\"../csv/CLA_meta_keywords.csv\")\n",
    "df_themes = pd.read_csv(\"../csv/CLA_themes.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('dutch')\n",
    "\n",
    "custom_stopwords=['waarvoor','wegens','sommige','betreffende']\n",
    "stopwords.extend(custom_stopwords)\n",
    "print (stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script creates a dataframe with the high ranked keywords per document\n",
    "\n",
    "from rake_nltk import Metric, Rake\n",
    "rake_nltk_var = Rake(language='dutch',stopwords=stopwords,include_repeated_phrases=False,ranking_metric=Metric.WORD_DEGREE,max_length=2)\n",
    "import re\n",
    "from os import listdir\n",
    "\n",
    "def make_targets_nlp():\n",
    "    input_path='../processed_data/NL'\n",
    "    file_list=listdir(input_path)\n",
    "\n",
    "    doc_list=[]\n",
    "    for file in file_list:\n",
    "        document_id=file[3:-4]\n",
    "        doc_list.append(document_id) \n",
    "\n",
    "    df_target_full=pd.DataFrame()\n",
    "    for document_id in doc_list:\n",
    "        # Get the themes_nl metadata for document_id in df_m\n",
    "        text=df_m.loc[df_m['filename'].str.contains(document_id),'themes_nl'].values.astype(str).tolist()\n",
    "    \n",
    "        # Check if value is filled and extract keywords\n",
    "        if pd.isnull(df_m.loc[df_m['filename'].str.contains(document_id),'themes_nl'].values)!=True:\n",
    "            text = re.sub(\"\\)\\\\\\\\\",\"\",text[0])\n",
    "            text = re.sub(\"\\)\\.\",\"\",text)\n",
    "            text = re.sub(\" \\)\",\"\",text)\n",
    "            rake_nltk_var.extract_keywords_from_text(text)\n",
    "            keyword_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "    \n",
    "\n",
    "        meta_theme=df_meta.loc[df_meta['filename'].str.contains(document_id),'themes_nl'].values[0]\n",
    "    \n",
    "        doc_keywords=[document_id] \n",
    "        targets=set(keyword_extracted)\n",
    "        doc_keywords=[document_id]\n",
    "\n",
    "        for x in targets:\n",
    "            doc_keywords.append(x)\n",
    "\n",
    "\n",
    "        df_target=pd.DataFrame(doc_keywords)\n",
    "        df_target=df_target.transpose()\n",
    "\n",
    "        df_target_full=pd.concat([df_target_full,df_target],ignore_index=True)\n",
    "\n",
    "    df_target_full.rename(columns = {0:'document_id'}, inplace = True)\n",
    "    df_target_full.reset_index(drop=True,inplace=True)\n",
    "    df_target_full.to_csv('../csv/document_targets.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the script\n",
    "\n",
    "#make_targets_nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def clean_text(text):\n",
    "  text = text.lower()\n",
    "  text = re.sub(\"[^a-zA-Z\\'\\-éôëè]\", \" \", text) \n",
    "  return \" \".join(word_tokenize(text)[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe for the model \n",
    "# and write it to document_targets.csv\n",
    "\n",
    "import os\n",
    "\n",
    "def make_nlp_model_df():\n",
    "    input_path=os.path.join('..','processed_data','NL')\n",
    "    file_list=listdir(input_path)\n",
    "    doc_list=[]\n",
    "\n",
    "    for file in file_list:\n",
    "        document_id=file[3:-4]\n",
    "        doc_list.append(document_id) \n",
    "\n",
    "    df_nlp_model=pd.read_csv('../csv/document_targets.csv',sep=';')\n",
    "\n",
    "    for document_id in doc_list:\n",
    "        with open(os.path.join(input_path,f'NL_{document_id}.txt'),encoding=\"utf-8\") as f:\n",
    "            text = f.read() \n",
    "            df_nlp_model.loc[df_nlp_model['document_id']==document_id,'text']=clean_text(text)\n",
    "\n",
    "    df_nlp_model.reset_index(drop=True,inplace=True)\n",
    "    df_nlp_model.to_csv('../csv/document_targets.csv', sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make_nlp_model_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nlp_model=pd.read_csv('../csv/document_targets.csv',sep=';')\n",
    "df_nlp_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check targets to themes from website\n",
    "\n",
    "df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_targets():\n",
    "    input_path='../processed_data/NL'\n",
    "    file_list=listdir(input_path)\n",
    "\n",
    "    doc_list=[]\n",
    "    for file in file_list:\n",
    "        document_id=file[3:-4]\n",
    "        doc_list.append(document_id) \n",
    "\n",
    "    df_target_full=pd.DataFrame()\n",
    "    for document_id in doc_list:\n",
    "        # Get the themes_nl metadata for document_id in df_m\n",
    "        text=df_m.loc[df_m['filename'].str.contains(document_id),'themes_nl'].values.astype(str).tolist()\n",
    "    \n",
    "        # Check if value is filled and extract keywords\n",
    "        if pd.isnull(df_m.loc[df_m['filename'].str.contains(document_id),'themes_nl'].values)!=True:\n",
    "            text = re.sub(\"\\)\\\\\\\\\",\"\",text[0])\n",
    "            text = re.sub(\"\\)\\.\",\"\",text)\n",
    "            text = re.sub(\" \\)\",\"\",text)\n",
    "            #rake_nltk_var.extract_keywords_from_text(text)\n",
    "            #keyword_extracted = rake_nltk_var.get_ranked_phrases()\n",
    "            #print (text)\n",
    "\n",
    "        meta_theme=df_meta.loc[df_meta['filename'].str.contains(document_id),'themes_nl'].values[0]\n",
    "        #print (meta_theme)    \n",
    "        targets=[]\n",
    "\n",
    "        text=str(text)\n",
    "        if len(text)>0:\n",
    "            text=re.sub('\\\\\\\\','@@',text)\n",
    "            while len(text)>0:\n",
    "                stop=text.find('@@')\n",
    "                #print (stop)\n",
    "                if stop == -1:\n",
    "                    stop = len (text)\n",
    "                target=text[0:stop]\n",
    "                text=text[stop+2:]\n",
    "                targets.append(target)\n",
    "\n",
    "        targets=set(targets)\n",
    "        doc_targets=[document_id]\n",
    "\n",
    "        for x in targets:\n",
    "            doc_targets.append(x)\n",
    "                         \n",
    "        print (doc_targets)\n",
    "        df_target=pd.DataFrame(doc_targets)\n",
    "        df_target=df_target.transpose()\n",
    "\n",
    "        df_target_full=pd.concat([df_target_full,df_target],ignore_index=True)\n",
    "\n",
    "    df_target_full.rename(columns = {0:'document_id'}, inplace = True)\n",
    "    df_target_full.reset_index(drop=True,inplace=True)\n",
    "    df_target_full.to_csv('../csv/NL_document_full_targets.csv', sep=\";\")\n",
    "\n",
    "#make_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataframe for the model \n",
    "# and write it to document_targets.csv\n",
    "\n",
    "import os\n",
    "\n",
    "def make_model_df():\n",
    "    input_path=os.path.join('..','processed_data','NL')\n",
    "    file_list=listdir(input_path)\n",
    "    doc_list=[]\n",
    "\n",
    "    for file in file_list:\n",
    "        document_id=file[3:-4]\n",
    "        doc_list.append(document_id) \n",
    "\n",
    "    df_model=pd.read_csv('../csv/NL_document_full_targets.csv',sep=';')\n",
    "\n",
    "    for document_id in doc_list:\n",
    "        with open(os.path.join(input_path,f'NL_{document_id}.txt'),encoding=\"utf-8\") as f:\n",
    "            text = f.read() \n",
    "            df_model.loc[df_nlp_model['document_id']==document_id,'text']=clean_text(text)\n",
    "\n",
    "    df_model.reset_index(drop=True,inplace=True)\n",
    "    df_model.to_csv('../csv/NL_document_full_targets.csv', sep=\";\", index=False)\n",
    "\n",
    "#make_model_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model=pd.read_csv('../csv/NL_document_full_targets.csv',sep=';')\n",
    "\n",
    "X = df_model[\"text\"].tolist()\n",
    "y = pd.get_dummies(df_model['1'])\n",
    "\n",
    "mapping = {i: name for i, name in enumerate(y.columns)}\n",
    "\n",
    "#print (mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Split Train and Validation data\n",
    "\n",
    "#print (len(X))\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "#print ('X_train_val',len(X_train_val))\n",
    "#print ('X_test',len(X_test))\n",
    "\n",
    "print (len(X))\n",
    "# Keep some data for inference (testing)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "print ('X_train',len(X_train))\n",
    "print ('X_test',len(X_test))\n",
    "print ('X_val',len(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(X_train, max_length=500, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(X_val, max_length=500, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(X_test, max_length=500, truncation=True, padding=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train\n",
    ")).batch(2)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val\n",
    ")).batch(2)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test\n",
    ")).batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFDistilBertForSequenceClassification\n",
    "\n",
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER =  tf.keras.optimizers.Adam(learning_rate=1)\n",
    "LOSS = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "METRICS = ['accuracy']\n",
    "\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=set(y)\n",
    "BATCH_SIZE = 8\n",
    "#model.fit(dict(train_dataset.batch(BATCH_SIZE)),dict(validation_data=val_dataset.batch(BATCH_SIZE)))\n",
    "#model.fit(train_dataset.batch(BATCH_SIZE))\n",
    "#model.fit(dict(train_encodings), y_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.run_functions_eagerly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "EPOCHS = 5\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "googenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9 (tags/v3.7.9:13c94747c7, Aug 17 2020, 18:58:18) [MSC v.1900 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "54a7e325ecc84fa625d8a3ed09038798785ca9ae88cae94e662f38988cc00c57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
